{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0065b58e",
   "metadata": {
    "id": "0065b58e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "random.seed(42)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d7d006",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "QYIONFNMTubl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYIONFNMTubl",
    "outputId": "4bed9165-ecbf-4dd3-914f-b6907322e406"
   },
   "outputs": [],
   "source": [
    "#Set path to folder containing train test file \n",
    "path = \"D:\\IIITD\\DL\\A1\\Fashion MNIST\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c0efeeb",
   "metadata": {
    "id": "6c0efeeb"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(path + \"fashion-mnist_train.csv\")\n",
    "X_test = pd.read_csv(path + \"fashion-mnist_test.csv\")\n",
    "Y_train = X_train.pop('label')\n",
    "Y_test = X_test.pop('label')\n",
    "X_train = np.asarray(X_train)\n",
    "X_test = np.asarray(X_test)\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_test = np.asarray(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd15538",
   "metadata": {
    "id": "3e40ff9b"
   },
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "273c985c",
   "metadata": {
    "id": "273c985c"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "988b55e4",
   "metadata": {
    "id": "988b55e4"
   },
   "outputs": [],
   "source": [
    "Y_train_enc = np.empty(shape=(Y_train.shape[0],10))\n",
    "for idx,i in zip(range(len(Y_train)),Y_train):\n",
    "    Y_train_enc[idx][i] = 1\n",
    "Y_test_enc = np.empty(shape=(Y_test.shape[0],10))\n",
    "for idx,i in zip(range(len(Y_test)),Y_test):\n",
    "    Y_test_enc[idx][i] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc670f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "417f4616",
    "outputId": "3154cfbf-fa30-465a-ca8c-5fe96fa14d12"
   },
   "source": [
    "# Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c0735d8",
   "metadata": {
    "id": "2c0735d8"
   },
   "outputs": [],
   "source": [
    "class network:\n",
    "    def __init__(self, n_hidden, i_inputs, o_outputs, activation_function):\n",
    "        self.n = len(n_hidden) # 512 512\n",
    "        \n",
    "        #List having structure of network\n",
    "        self.n_layers = n_hidden\n",
    "        self.i = i_inputs\n",
    "        self.o = o_outputs\n",
    "        self.choice = activation_function\n",
    "        \n",
    "        #add output layer to structure\n",
    "        self.n_layers.append(self.o)\n",
    "        #Weights of all layers stored as list\n",
    "        self.weights = [] \n",
    "        #Bias of all layers stored as list\n",
    "        self.bias = []\n",
    "        \n",
    "        #Weight initialization for hidden layer\n",
    "        next_input = self.i\n",
    "        for n_neurons in n_hidden:\n",
    "            w = np.random.randn(n_neurons, next_input)*0.01\n",
    "            self.weights.append(w)\n",
    "            next_input = n_neurons\n",
    "        #Weights for output layer\n",
    "        self.weights.append(np.random.randn(self.o, next_input)*0.01)\n",
    "        #Bias initialization for hidden layer\n",
    "        for n_neurons in n_hidden:\n",
    "            b = np.random.rand(n_neurons,1)*0.01\n",
    "            self.bias.append(b)\n",
    "        #Bias for output layer\n",
    "        self.bias.append(np.random.rand(self.o,1)*0.01)\n",
    "        \n",
    "        \n",
    "    def activation(self, X, choice = 0):\n",
    "        #Sigmoid\n",
    "        if choice == 0:  \n",
    "            return 1.0 /(1.0 + np.exp(-X))\n",
    "        #TanH\n",
    "        elif choice == 1:\n",
    "            return np.tanh(X)\n",
    "        #ReLu\n",
    "        elif choice == 2: \n",
    "            return np.maximum(X, 0) \n",
    "        #Softmax\n",
    "        elif choice == 3:\n",
    "            expX = np.exp(X) #,dtype=np.float128)  # WILL ONLY WORK ON COLAB. BUT NEEDED TO AVOID OVERFLOW\n",
    "            return expX/np.sum(expX, axis = 0)\n",
    "        \n",
    "    \n",
    "    def derivative(self, X, choice = 0):\n",
    "        #Sigmoid\n",
    "        if choice == 0: \n",
    "            d = self.activation(X, 0)\n",
    "            return (d*(1-d))\n",
    "        #TanH\n",
    "        elif choice == 1:\n",
    "            return (1 - np.power(np.tanh(X), 2))\n",
    "        #ReLu\n",
    "        elif choice == 2: \n",
    "            return np.array(X > 0, dtype = np.float32)\n",
    "    \n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        self.z = []\n",
    "        self.a = []\n",
    "        next_input = X.T\n",
    "        #Traverse n hidden layers + 1 output layer\n",
    "        for i in range(self.n+1):\n",
    "            z = np.dot(self.weights[i], next_input) + self.bias[i]\n",
    "            #SOFTMAX\n",
    "            if i == self.n:\n",
    "                a = self.activation(z,3)\n",
    "            #Activation function of choice\n",
    "            else:\n",
    "                a = self.activation(z,self.choice)\n",
    "            next_input = a\n",
    "            self.z.append(z)\n",
    "            self.a.append(a)\n",
    "        return next_input\n",
    "    \n",
    "    def loss_function(self, op, Y):\n",
    "        #Calculates cross entropy loss\n",
    "        m = Y.shape[0]\n",
    "        loss = -(1/m)*np.sum(Y.T*np.log(op)) \n",
    "        return loss\n",
    "        \n",
    "    def backward_prop(self, X, Y):\n",
    "        n = X.shape[0]\n",
    "        self.weight_gradients = []\n",
    "        self.bias_gradients = []\n",
    "        \n",
    "        #Gradient Calculation for Output layer\n",
    "        dz = self.a[-1] - Y.T\n",
    "        dw = (1/n)*np.dot(dz, self.a[-2].T)\n",
    "        db = (1/n)*np.sum(dz, axis = 1, keepdims = True)\n",
    "        self.weight_gradients.append(dw)\n",
    "        self.bias_gradients.append(db)\n",
    "        #Gradients for Hidden layers\n",
    "        for i in reversed(range(self.n)):\n",
    "            dz = (1/n)*np.dot(self.weights[i+1].T, dz)*self.derivative(self.a[i], self.choice)\n",
    "            if i == 0:\n",
    "                dw = (1/n)*np.dot(dz, X)\n",
    "            else:\n",
    "                dw = (1/n)*np.dot(dz, self.z[i-1].T)\n",
    "            db = (1/n)*np.sum(dz, axis = 1, keepdims = True)\n",
    "            self.weight_gradients.append(dw)\n",
    "            self.bias_gradients.append(db)\n",
    "        self.weight_gradients.reverse()\n",
    "        self.bias_gradients.reverse()\n",
    "    \n",
    "    #GRADIENT DESCENT\n",
    "    def grad_desc(self, learning_rate):\n",
    "        for i in range(self.n+1):\n",
    "            self.weights[i] -= learning_rate * self.weight_gradients[i]\n",
    "            self.bias[i] -= learning_rate * self.bias_gradients[i]\n",
    "            \n",
    "    #GRADIENT DESCENT WITH MOMENTUM\n",
    "    def grad_desc_moment(self, learning_rate, momentum_weight):\n",
    "        Vw = []\n",
    "        Vb = []\n",
    "        for i in range(self.n+1):\n",
    "            Vw.append(np.zeros(self.weight_gradients[i].shape))\n",
    "            Vb.append(np.zeros(self.bias_gradients[i].shape))\n",
    "        \n",
    "        for i in range(self.n+1):\n",
    "            Vw[i] = momentum_weight * Vw[i] + (1 - momentum_weight) * self.weight_gradients[i]\n",
    "            Vb[i] = momentum_weight * Vb[i] + (1 - momentum_weight) * self.bias_gradients[i]\n",
    "            self.weights[i] -= learning_rate * Vw[i]\n",
    "            self.bias[i] -= learning_rate * Vb[i]\n",
    "            \n",
    "    #NESTEROV ACCELERATED GRADIENT\n",
    "    def NAG(self, learning_rate, momentum_weight, X, Y):\n",
    "        w = []\n",
    "        b = []\n",
    "        for i in range(self.n+1):\n",
    "            w.append(self.weights[i])\n",
    "            b.append(self.bias[i])\n",
    "            self.weights[i] = self.weights[i] - momentum_weight * self.Vw[i]\n",
    "            self.bias[i] = self.bias[i] - momentum_weight * self.Vb[i]\n",
    "\n",
    "        op = self.forward_prop(X)\n",
    "        self.backward_prop(X, Y)\n",
    "\n",
    "        for i in range(self.n+1):\n",
    "            self.Vw[i] = momentum_weight * self.Vw[i] + learning_rate * self.weight_gradients[i]\n",
    "            self.Vb[i] = momentum_weight * self.Vb[i] + learning_rate * self.bias_gradients[i]\n",
    "            self.weights[i] = w[i] - self.Vw[i]\n",
    "            self.bias[i] = b[i] - self.Vb[i]\n",
    "            \n",
    "    #ROOT MEAN SQUARE PROPOGATION\n",
    "    def RMSProp(self, learning_rate, beta = 0.9):\n",
    "        Sw = []\n",
    "        Sb = []\n",
    "        for i in range(self.n+1):\n",
    "            Sw.append(np.zeros(self.weight_gradients[i].shape))\n",
    "            Sb.append(np.zeros(self.bias_gradients[i].shape))\n",
    "\n",
    "        for i in range(self.n+1):\n",
    "            Sw[i] = beta * Sw[i] + (1 - beta) * (self.weight_gradients[i] ** 2)\n",
    "            Sw[i] += 1e-8\n",
    "            Sb[i] = beta * Sb[i] + (1 - beta) * (self.bias_gradients[i] ** 2)\n",
    "            Sb[i] += 1e-8\n",
    "            self.weights[i] -= learning_rate * (self.weight_gradients[i]/np.sqrt(Sw[i]))\n",
    "            self.bias[i] -= learning_rate * (self.bias_gradients[i]/np.sqrt(Sb[i]))\n",
    "    \n",
    "    #ADAPTIVE GRADIENTS\n",
    "    def Adagrad(self, learning_rate):\n",
    "        Gw = []\n",
    "        Gb = []\n",
    "        for i in range(self.n+1):     \n",
    "            Gw.append(np.zeros(self.weight_gradients[i].shape))\n",
    "            Gb.append(np.zeros(self.bias_gradients[i].shape))\n",
    "\n",
    "        for i in range(self.n+1):\n",
    "            Gw[i] += (self.weight_gradients[i] ** 2)\n",
    "            Gw[i] += 1e-8\n",
    "            Gb[i] = (self.bias_gradients[i] ** 2)\n",
    "            Gb[i] += 1e-8\n",
    "            self.weights[i] -= learning_rate * (self.weight_gradients[i]/np.sqrt(Gw[i]))\n",
    "            self.bias[i] -= learning_rate * (self.bias_gradients[i]/np.sqrt(Gb[i]))\n",
    "\n",
    "    def Adam(self, learning_rate, beta1 = 0.9, beta2 = 0.999):\n",
    "        Sw = []\n",
    "        Sb = []\n",
    "        Vw = []\n",
    "        Vb = []\n",
    "        for i in range(self.n+1):\n",
    "            \n",
    "            Sw.append(np.zeros(self.weight_gradients[i].shape))\n",
    "            Sb.append(np.zeros(self.bias_gradients[i].shape))\n",
    "            Vw.append(np.zeros(self.weight_gradients[i].shape))\n",
    "            Vb.append(np.zeros(self.bias_gradients[i].shape))\n",
    "\n",
    "        for i in range(self.n+1):\n",
    "            Vw[i] = beta1 * Vw[i] + (1 - beta1) * self.weight_gradients[i]\n",
    "            Vb[i] = beta1 * Vb[i] + (1 - beta1) * self.bias_gradients[i]\n",
    "\n",
    "            Sw[i] = beta2 * Sw[i] + (1 - beta2) * (self.weight_gradients[i] ** 2)\n",
    "            Sw[i] += 1e-8\n",
    "            Sb[i] = beta2 * Sb[i] + (1 - beta2) * (self.bias_gradients[i] ** 2)\n",
    "            Sb[i] += 1e-8\n",
    "            self.weights[i] -= learning_rate * (Vw[i]/np.sqrt(Sw[i]))\n",
    "            self.bias[i] -= learning_rate * (Vb[i]/np.sqrt(Sb[i]))\n",
    "        \n",
    "    def train(self, X, Y, learning_rate, n_epoch, batch_size, optimizer = 0, momentum_weight = 0.9):\n",
    "        self.loss = {}\n",
    "        for epoch in range(n_epoch):\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X[i:i + batch_size]\n",
    "                Y_batch = Y[i:i + batch_size]\n",
    "                op = self.forward_prop(X_batch)\n",
    "                self.backward_prop(X_batch, Y_batch)\n",
    "                if optimizer == 0:#gradient_desc\n",
    "                    self.grad_desc(learning_rate)\n",
    "                \n",
    "                elif optimizer == 1:#gradient_desc with momentum\n",
    "                    self.grad_desc_moment(learning_rate, momentum_weight)\n",
    "\n",
    "                elif optimizer == 2:#NAG\n",
    "                    self.NAG(learning_rate, momentum_weight, X_batch, Y_batch)\n",
    "\n",
    "                elif optimizer == 3:#RMSprop\n",
    "                    self.RMSProp(learning_rate)\n",
    "\n",
    "                elif optimizer == 4:#Adagrad\n",
    "                    self.Adagrad(learning_rate)\n",
    "\n",
    "                elif optimizer == 5:#Adam\n",
    "                    self.Adam(learning_rate)\n",
    "\n",
    "            op = self.forward_prop(X)    \n",
    "            loss = self.loss_function(op, Y)\n",
    "            self.loss[epoch] = loss\n",
    "            print('epoch=%d, lrate=%.3f, loss=%.3f' % (epoch, learning_rate, loss))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        output = self.forward_prop(X)\n",
    "        return np.argmax(output, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a052211",
   "metadata": {},
   "source": [
    "# Constructing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2dc0adab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dc0adab",
    "outputId": "aae82ce0-2b9f-4770-cacb-12d057aa2337"
   },
   "outputs": [],
   "source": [
    "network_arch = [256]    #List representing neurons in each hidden layer eg. [128, 64]\n",
    "\n",
    "# ACTIVATION FUNCTION CHOICES:\n",
    "# 0: SIGMOID, 1: TANH, 2: RELU\n",
    "activation_function = 2  \n",
    "\n",
    "n_inputs = 784 #Features in dataset\n",
    "o_outputs = 10 #Classes in target label\n",
    "\n",
    "#Build network\n",
    "net = network(network_arch, n_inputs, o_outputs, activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92647902",
   "metadata": {},
   "source": [
    "# Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "385e535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, lrate=0.100, loss=2.302\n",
      "epoch=1, lrate=0.100, loss=2.299\n",
      "epoch=2, lrate=0.100, loss=2.295\n",
      "epoch=3, lrate=0.100, loss=2.292\n",
      "epoch=4, lrate=0.100, loss=2.289\n",
      "epoch=5, lrate=0.100, loss=2.286\n",
      "epoch=6, lrate=0.100, loss=2.283\n",
      "epoch=7, lrate=0.100, loss=2.280\n",
      "epoch=8, lrate=0.100, loss=2.277\n",
      "epoch=9, lrate=0.100, loss=2.274\n",
      "epoch=10, lrate=0.100, loss=2.271\n",
      "epoch=11, lrate=0.100, loss=2.268\n",
      "epoch=12, lrate=0.100, loss=2.265\n",
      "epoch=13, lrate=0.100, loss=2.262\n",
      "epoch=14, lrate=0.100, loss=2.259\n",
      "epoch=15, lrate=0.100, loss=2.256\n",
      "epoch=16, lrate=0.100, loss=2.254\n",
      "epoch=17, lrate=0.100, loss=2.251\n",
      "epoch=18, lrate=0.100, loss=2.248\n",
      "epoch=19, lrate=0.100, loss=2.245\n",
      "epoch=20, lrate=0.100, loss=2.243\n",
      "epoch=21, lrate=0.100, loss=2.240\n",
      "epoch=22, lrate=0.100, loss=2.237\n",
      "epoch=23, lrate=0.100, loss=2.235\n",
      "epoch=24, lrate=0.100, loss=2.232\n",
      "epoch=25, lrate=0.100, loss=2.229\n",
      "epoch=26, lrate=0.100, loss=2.227\n",
      "epoch=27, lrate=0.100, loss=2.224\n",
      "epoch=28, lrate=0.100, loss=2.222\n",
      "epoch=29, lrate=0.100, loss=2.219\n",
      "epoch=30, lrate=0.100, loss=2.216\n",
      "epoch=31, lrate=0.100, loss=2.214\n",
      "epoch=32, lrate=0.100, loss=2.211\n",
      "epoch=33, lrate=0.100, loss=2.209\n",
      "epoch=34, lrate=0.100, loss=2.206\n",
      "epoch=35, lrate=0.100, loss=2.204\n",
      "epoch=36, lrate=0.100, loss=2.201\n",
      "epoch=37, lrate=0.100, loss=2.199\n",
      "epoch=38, lrate=0.100, loss=2.197\n",
      "epoch=39, lrate=0.100, loss=2.194\n",
      "epoch=40, lrate=0.100, loss=2.192\n",
      "epoch=41, lrate=0.100, loss=2.189\n",
      "epoch=42, lrate=0.100, loss=2.187\n",
      "epoch=43, lrate=0.100, loss=2.184\n",
      "epoch=44, lrate=0.100, loss=2.182\n",
      "epoch=45, lrate=0.100, loss=2.180\n",
      "epoch=46, lrate=0.100, loss=2.177\n",
      "epoch=47, lrate=0.100, loss=2.175\n",
      "epoch=48, lrate=0.100, loss=2.173\n",
      "epoch=49, lrate=0.100, loss=2.170\n",
      "epoch=50, lrate=0.100, loss=2.168\n",
      "epoch=51, lrate=0.100, loss=2.165\n",
      "epoch=52, lrate=0.100, loss=2.163\n",
      "epoch=53, lrate=0.100, loss=2.161\n",
      "epoch=54, lrate=0.100, loss=2.159\n",
      "epoch=55, lrate=0.100, loss=2.156\n",
      "epoch=56, lrate=0.100, loss=2.154\n",
      "epoch=57, lrate=0.100, loss=2.152\n",
      "epoch=58, lrate=0.100, loss=2.149\n",
      "epoch=59, lrate=0.100, loss=2.147\n",
      "epoch=60, lrate=0.100, loss=2.145\n",
      "epoch=61, lrate=0.100, loss=2.143\n",
      "epoch=62, lrate=0.100, loss=2.140\n",
      "epoch=63, lrate=0.100, loss=2.138\n",
      "epoch=64, lrate=0.100, loss=2.136\n",
      "epoch=65, lrate=0.100, loss=2.134\n",
      "epoch=66, lrate=0.100, loss=2.131\n",
      "epoch=67, lrate=0.100, loss=2.129\n",
      "epoch=68, lrate=0.100, loss=2.127\n",
      "epoch=69, lrate=0.100, loss=2.125\n",
      "epoch=70, lrate=0.100, loss=2.122\n",
      "epoch=71, lrate=0.100, loss=2.120\n",
      "epoch=72, lrate=0.100, loss=2.118\n",
      "epoch=73, lrate=0.100, loss=2.116\n",
      "epoch=74, lrate=0.100, loss=2.114\n",
      "epoch=75, lrate=0.100, loss=2.112\n",
      "epoch=76, lrate=0.100, loss=2.109\n",
      "epoch=77, lrate=0.100, loss=2.107\n",
      "epoch=78, lrate=0.100, loss=2.105\n",
      "epoch=79, lrate=0.100, loss=2.103\n",
      "epoch=80, lrate=0.100, loss=2.101\n",
      "epoch=81, lrate=0.100, loss=2.099\n",
      "epoch=82, lrate=0.100, loss=2.097\n",
      "epoch=83, lrate=0.100, loss=2.094\n",
      "epoch=84, lrate=0.100, loss=2.092\n",
      "epoch=85, lrate=0.100, loss=2.090\n",
      "epoch=86, lrate=0.100, loss=2.088\n",
      "epoch=87, lrate=0.100, loss=2.086\n",
      "epoch=88, lrate=0.100, loss=2.084\n",
      "epoch=89, lrate=0.100, loss=2.082\n",
      "epoch=90, lrate=0.100, loss=2.080\n",
      "epoch=91, lrate=0.100, loss=2.078\n",
      "epoch=92, lrate=0.100, loss=2.075\n",
      "epoch=93, lrate=0.100, loss=2.073\n",
      "epoch=94, lrate=0.100, loss=2.071\n",
      "epoch=95, lrate=0.100, loss=2.069\n",
      "epoch=96, lrate=0.100, loss=2.067\n",
      "epoch=97, lrate=0.100, loss=2.065\n",
      "epoch=98, lrate=0.100, loss=2.063\n",
      "epoch=99, lrate=0.100, loss=2.061\n",
      "epoch=100, lrate=0.100, loss=2.059\n",
      "epoch=101, lrate=0.100, loss=2.057\n",
      "epoch=102, lrate=0.100, loss=2.055\n",
      "epoch=103, lrate=0.100, loss=2.053\n",
      "epoch=104, lrate=0.100, loss=2.051\n",
      "epoch=105, lrate=0.100, loss=2.049\n",
      "epoch=106, lrate=0.100, loss=2.047\n",
      "epoch=107, lrate=0.100, loss=2.045\n",
      "epoch=108, lrate=0.100, loss=2.043\n",
      "epoch=109, lrate=0.100, loss=2.041\n",
      "epoch=110, lrate=0.100, loss=2.039\n",
      "epoch=111, lrate=0.100, loss=2.037\n",
      "epoch=112, lrate=0.100, loss=2.035\n",
      "epoch=113, lrate=0.100, loss=2.033\n",
      "epoch=114, lrate=0.100, loss=2.031\n",
      "epoch=115, lrate=0.100, loss=2.029\n",
      "epoch=116, lrate=0.100, loss=2.027\n",
      "epoch=117, lrate=0.100, loss=2.025\n",
      "epoch=118, lrate=0.100, loss=2.023\n",
      "epoch=119, lrate=0.100, loss=2.021\n",
      "epoch=120, lrate=0.100, loss=2.019\n",
      "epoch=121, lrate=0.100, loss=2.017\n",
      "epoch=122, lrate=0.100, loss=2.016\n",
      "epoch=123, lrate=0.100, loss=2.014\n",
      "epoch=124, lrate=0.100, loss=2.012\n",
      "epoch=125, lrate=0.100, loss=2.010\n",
      "epoch=126, lrate=0.100, loss=2.008\n",
      "epoch=127, lrate=0.100, loss=2.006\n",
      "epoch=128, lrate=0.100, loss=2.004\n",
      "epoch=129, lrate=0.100, loss=2.002\n",
      "epoch=130, lrate=0.100, loss=2.000\n",
      "epoch=131, lrate=0.100, loss=1.998\n",
      "epoch=132, lrate=0.100, loss=1.996\n",
      "epoch=133, lrate=0.100, loss=1.995\n",
      "epoch=134, lrate=0.100, loss=1.993\n",
      "epoch=135, lrate=0.100, loss=1.991\n",
      "epoch=136, lrate=0.100, loss=1.989\n",
      "epoch=137, lrate=0.100, loss=1.987\n",
      "epoch=138, lrate=0.100, loss=1.985\n",
      "epoch=139, lrate=0.100, loss=1.983\n",
      "epoch=140, lrate=0.100, loss=1.982\n",
      "epoch=141, lrate=0.100, loss=1.980\n",
      "epoch=142, lrate=0.100, loss=1.978\n",
      "epoch=143, lrate=0.100, loss=1.976\n",
      "epoch=144, lrate=0.100, loss=1.974\n",
      "epoch=145, lrate=0.100, loss=1.972\n",
      "epoch=146, lrate=0.100, loss=1.971\n",
      "epoch=147, lrate=0.100, loss=1.969\n",
      "epoch=148, lrate=0.100, loss=1.967\n",
      "epoch=149, lrate=0.100, loss=1.965\n",
      "epoch=150, lrate=0.100, loss=1.963\n",
      "epoch=151, lrate=0.100, loss=1.962\n",
      "epoch=152, lrate=0.100, loss=1.960\n",
      "epoch=153, lrate=0.100, loss=1.958\n",
      "epoch=154, lrate=0.100, loss=1.956\n",
      "epoch=155, lrate=0.100, loss=1.954\n",
      "epoch=156, lrate=0.100, loss=1.953\n",
      "epoch=157, lrate=0.100, loss=1.951\n",
      "epoch=158, lrate=0.100, loss=1.949\n",
      "epoch=159, lrate=0.100, loss=1.947\n",
      "epoch=160, lrate=0.100, loss=1.946\n",
      "epoch=161, lrate=0.100, loss=1.944\n",
      "epoch=162, lrate=0.100, loss=1.942\n",
      "epoch=163, lrate=0.100, loss=1.940\n",
      "epoch=164, lrate=0.100, loss=1.939\n",
      "epoch=165, lrate=0.100, loss=1.937\n",
      "epoch=166, lrate=0.100, loss=1.935\n",
      "epoch=167, lrate=0.100, loss=1.933\n",
      "epoch=168, lrate=0.100, loss=1.932\n",
      "epoch=169, lrate=0.100, loss=1.930\n",
      "epoch=170, lrate=0.100, loss=1.928\n",
      "epoch=171, lrate=0.100, loss=1.927\n",
      "epoch=172, lrate=0.100, loss=1.925\n",
      "epoch=173, lrate=0.100, loss=1.923\n",
      "epoch=174, lrate=0.100, loss=1.921\n",
      "epoch=175, lrate=0.100, loss=1.920\n",
      "epoch=176, lrate=0.100, loss=1.918\n",
      "epoch=177, lrate=0.100, loss=1.916\n",
      "epoch=178, lrate=0.100, loss=1.915\n",
      "epoch=179, lrate=0.100, loss=1.913\n",
      "epoch=180, lrate=0.100, loss=1.911\n",
      "epoch=181, lrate=0.100, loss=1.910\n",
      "epoch=182, lrate=0.100, loss=1.908\n",
      "epoch=183, lrate=0.100, loss=1.906\n",
      "epoch=184, lrate=0.100, loss=1.905\n",
      "epoch=185, lrate=0.100, loss=1.903\n",
      "epoch=186, lrate=0.100, loss=1.901\n",
      "epoch=187, lrate=0.100, loss=1.900\n",
      "epoch=188, lrate=0.100, loss=1.898\n",
      "epoch=189, lrate=0.100, loss=1.896\n",
      "epoch=190, lrate=0.100, loss=1.895\n",
      "epoch=191, lrate=0.100, loss=1.893\n",
      "epoch=192, lrate=0.100, loss=1.891\n",
      "epoch=193, lrate=0.100, loss=1.890\n",
      "epoch=194, lrate=0.100, loss=1.888\n",
      "epoch=195, lrate=0.100, loss=1.887\n",
      "epoch=196, lrate=0.100, loss=1.885\n",
      "epoch=197, lrate=0.100, loss=1.883\n",
      "epoch=198, lrate=0.100, loss=1.882\n",
      "epoch=199, lrate=0.100, loss=1.880\n",
      "epoch=200, lrate=0.100, loss=1.879\n",
      "epoch=201, lrate=0.100, loss=1.877\n",
      "epoch=202, lrate=0.100, loss=1.875\n",
      "epoch=203, lrate=0.100, loss=1.874\n",
      "epoch=204, lrate=0.100, loss=1.872\n",
      "epoch=205, lrate=0.100, loss=1.871\n",
      "epoch=206, lrate=0.100, loss=1.869\n",
      "epoch=207, lrate=0.100, loss=1.867\n",
      "epoch=208, lrate=0.100, loss=1.866\n",
      "epoch=209, lrate=0.100, loss=1.864\n",
      "epoch=210, lrate=0.100, loss=1.863\n",
      "epoch=211, lrate=0.100, loss=1.861\n",
      "epoch=212, lrate=0.100, loss=1.860\n",
      "epoch=213, lrate=0.100, loss=1.858\n",
      "epoch=214, lrate=0.100, loss=1.857\n",
      "epoch=215, lrate=0.100, loss=1.855\n",
      "epoch=216, lrate=0.100, loss=1.853\n",
      "epoch=217, lrate=0.100, loss=1.852\n",
      "epoch=218, lrate=0.100, loss=1.850\n",
      "epoch=219, lrate=0.100, loss=1.849\n",
      "epoch=220, lrate=0.100, loss=1.847\n",
      "epoch=221, lrate=0.100, loss=1.846\n",
      "epoch=222, lrate=0.100, loss=1.844\n",
      "epoch=223, lrate=0.100, loss=1.843\n",
      "epoch=224, lrate=0.100, loss=1.841\n",
      "epoch=225, lrate=0.100, loss=1.840\n",
      "epoch=226, lrate=0.100, loss=1.838\n",
      "epoch=227, lrate=0.100, loss=1.837\n",
      "epoch=228, lrate=0.100, loss=1.835\n",
      "epoch=229, lrate=0.100, loss=1.834\n",
      "epoch=230, lrate=0.100, loss=1.832\n",
      "epoch=231, lrate=0.100, loss=1.831\n",
      "epoch=232, lrate=0.100, loss=1.829\n",
      "epoch=233, lrate=0.100, loss=1.828\n",
      "epoch=234, lrate=0.100, loss=1.826\n",
      "epoch=235, lrate=0.100, loss=1.825\n",
      "epoch=236, lrate=0.100, loss=1.823\n",
      "epoch=237, lrate=0.100, loss=1.822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=238, lrate=0.100, loss=1.820\n",
      "epoch=239, lrate=0.100, loss=1.819\n",
      "epoch=240, lrate=0.100, loss=1.817\n",
      "epoch=241, lrate=0.100, loss=1.816\n",
      "epoch=242, lrate=0.100, loss=1.815\n",
      "epoch=243, lrate=0.100, loss=1.813\n",
      "epoch=244, lrate=0.100, loss=1.812\n",
      "epoch=245, lrate=0.100, loss=1.810\n",
      "epoch=246, lrate=0.100, loss=1.809\n",
      "epoch=247, lrate=0.100, loss=1.807\n",
      "epoch=248, lrate=0.100, loss=1.806\n",
      "epoch=249, lrate=0.100, loss=1.804\n",
      "epoch=250, lrate=0.100, loss=1.803\n",
      "epoch=251, lrate=0.100, loss=1.802\n",
      "epoch=252, lrate=0.100, loss=1.800\n",
      "epoch=253, lrate=0.100, loss=1.799\n",
      "epoch=254, lrate=0.100, loss=1.797\n",
      "epoch=255, lrate=0.100, loss=1.796\n",
      "epoch=256, lrate=0.100, loss=1.794\n",
      "epoch=257, lrate=0.100, loss=1.793\n",
      "epoch=258, lrate=0.100, loss=1.792\n",
      "epoch=259, lrate=0.100, loss=1.790\n",
      "epoch=260, lrate=0.100, loss=1.789\n",
      "epoch=261, lrate=0.100, loss=1.787\n",
      "epoch=262, lrate=0.100, loss=1.786\n",
      "epoch=263, lrate=0.100, loss=1.785\n",
      "epoch=264, lrate=0.100, loss=1.783\n",
      "epoch=265, lrate=0.100, loss=1.782\n",
      "epoch=266, lrate=0.100, loss=1.781\n",
      "epoch=267, lrate=0.100, loss=1.779\n",
      "epoch=268, lrate=0.100, loss=1.778\n",
      "epoch=269, lrate=0.100, loss=1.776\n",
      "epoch=270, lrate=0.100, loss=1.775\n",
      "epoch=271, lrate=0.100, loss=1.774\n",
      "epoch=272, lrate=0.100, loss=1.772\n",
      "epoch=273, lrate=0.100, loss=1.771\n",
      "epoch=274, lrate=0.100, loss=1.770\n",
      "epoch=275, lrate=0.100, loss=1.768\n",
      "epoch=276, lrate=0.100, loss=1.767\n",
      "epoch=277, lrate=0.100, loss=1.766\n",
      "epoch=278, lrate=0.100, loss=1.764\n",
      "epoch=279, lrate=0.100, loss=1.763\n",
      "epoch=280, lrate=0.100, loss=1.762\n",
      "epoch=281, lrate=0.100, loss=1.760\n",
      "epoch=282, lrate=0.100, loss=1.759\n",
      "epoch=283, lrate=0.100, loss=1.758\n",
      "epoch=284, lrate=0.100, loss=1.756\n",
      "epoch=285, lrate=0.100, loss=1.755\n",
      "epoch=286, lrate=0.100, loss=1.754\n",
      "epoch=287, lrate=0.100, loss=1.752\n",
      "epoch=288, lrate=0.100, loss=1.751\n",
      "epoch=289, lrate=0.100, loss=1.750\n",
      "epoch=290, lrate=0.100, loss=1.748\n",
      "epoch=291, lrate=0.100, loss=1.747\n",
      "epoch=292, lrate=0.100, loss=1.746\n",
      "epoch=293, lrate=0.100, loss=1.744\n",
      "epoch=294, lrate=0.100, loss=1.743\n",
      "epoch=295, lrate=0.100, loss=1.742\n",
      "epoch=296, lrate=0.100, loss=1.741\n",
      "epoch=297, lrate=0.100, loss=1.739\n",
      "epoch=298, lrate=0.100, loss=1.738\n",
      "epoch=299, lrate=0.100, loss=1.737\n",
      "epoch=300, lrate=0.100, loss=1.735\n",
      "epoch=301, lrate=0.100, loss=1.734\n",
      "epoch=302, lrate=0.100, loss=1.733\n",
      "epoch=303, lrate=0.100, loss=1.732\n",
      "epoch=304, lrate=0.100, loss=1.730\n",
      "epoch=305, lrate=0.100, loss=1.729\n",
      "epoch=306, lrate=0.100, loss=1.728\n",
      "epoch=307, lrate=0.100, loss=1.727\n",
      "epoch=308, lrate=0.100, loss=1.725\n",
      "epoch=309, lrate=0.100, loss=1.724\n",
      "epoch=310, lrate=0.100, loss=1.723\n",
      "epoch=311, lrate=0.100, loss=1.722\n",
      "epoch=312, lrate=0.100, loss=1.720\n",
      "epoch=313, lrate=0.100, loss=1.719\n",
      "epoch=314, lrate=0.100, loss=1.718\n",
      "epoch=315, lrate=0.100, loss=1.717\n",
      "epoch=316, lrate=0.100, loss=1.715\n",
      "epoch=317, lrate=0.100, loss=1.714\n",
      "epoch=318, lrate=0.100, loss=1.713\n",
      "epoch=319, lrate=0.100, loss=1.712\n",
      "epoch=320, lrate=0.100, loss=1.710\n",
      "epoch=321, lrate=0.100, loss=1.709\n",
      "epoch=322, lrate=0.100, loss=1.708\n",
      "epoch=323, lrate=0.100, loss=1.707\n",
      "epoch=324, lrate=0.100, loss=1.706\n",
      "epoch=325, lrate=0.100, loss=1.704\n",
      "epoch=326, lrate=0.100, loss=1.703\n",
      "epoch=327, lrate=0.100, loss=1.702\n",
      "epoch=328, lrate=0.100, loss=1.701\n",
      "epoch=329, lrate=0.100, loss=1.700\n",
      "epoch=330, lrate=0.100, loss=1.698\n",
      "epoch=331, lrate=0.100, loss=1.697\n",
      "epoch=332, lrate=0.100, loss=1.696\n",
      "epoch=333, lrate=0.100, loss=1.695\n",
      "epoch=334, lrate=0.100, loss=1.694\n",
      "epoch=335, lrate=0.100, loss=1.692\n",
      "epoch=336, lrate=0.100, loss=1.691\n",
      "epoch=337, lrate=0.100, loss=1.690\n",
      "epoch=338, lrate=0.100, loss=1.689\n",
      "epoch=339, lrate=0.100, loss=1.688\n",
      "epoch=340, lrate=0.100, loss=1.687\n",
      "epoch=341, lrate=0.100, loss=1.685\n",
      "epoch=342, lrate=0.100, loss=1.684\n",
      "epoch=343, lrate=0.100, loss=1.683\n",
      "epoch=344, lrate=0.100, loss=1.682\n",
      "epoch=345, lrate=0.100, loss=1.681\n",
      "epoch=346, lrate=0.100, loss=1.680\n",
      "epoch=347, lrate=0.100, loss=1.678\n",
      "epoch=348, lrate=0.100, loss=1.677\n",
      "epoch=349, lrate=0.100, loss=1.676\n",
      "epoch=350, lrate=0.100, loss=1.675\n",
      "epoch=351, lrate=0.100, loss=1.674\n",
      "epoch=352, lrate=0.100, loss=1.673\n",
      "epoch=353, lrate=0.100, loss=1.671\n",
      "epoch=354, lrate=0.100, loss=1.670\n",
      "epoch=355, lrate=0.100, loss=1.669\n",
      "epoch=356, lrate=0.100, loss=1.668\n",
      "epoch=357, lrate=0.100, loss=1.667\n",
      "epoch=358, lrate=0.100, loss=1.666\n",
      "epoch=359, lrate=0.100, loss=1.665\n",
      "epoch=360, lrate=0.100, loss=1.664\n",
      "epoch=361, lrate=0.100, loss=1.662\n",
      "epoch=362, lrate=0.100, loss=1.661\n",
      "epoch=363, lrate=0.100, loss=1.660\n",
      "epoch=364, lrate=0.100, loss=1.659\n",
      "epoch=365, lrate=0.100, loss=1.658\n",
      "epoch=366, lrate=0.100, loss=1.657\n",
      "epoch=367, lrate=0.100, loss=1.656\n",
      "epoch=368, lrate=0.100, loss=1.655\n",
      "epoch=369, lrate=0.100, loss=1.653\n",
      "epoch=370, lrate=0.100, loss=1.652\n",
      "epoch=371, lrate=0.100, loss=1.651\n",
      "epoch=372, lrate=0.100, loss=1.650\n",
      "epoch=373, lrate=0.100, loss=1.649\n",
      "epoch=374, lrate=0.100, loss=1.648\n",
      "epoch=375, lrate=0.100, loss=1.647\n",
      "epoch=376, lrate=0.100, loss=1.646\n",
      "epoch=377, lrate=0.100, loss=1.645\n",
      "epoch=378, lrate=0.100, loss=1.644\n",
      "epoch=379, lrate=0.100, loss=1.643\n",
      "epoch=380, lrate=0.100, loss=1.641\n",
      "epoch=381, lrate=0.100, loss=1.640\n",
      "epoch=382, lrate=0.100, loss=1.639\n",
      "epoch=383, lrate=0.100, loss=1.638\n",
      "epoch=384, lrate=0.100, loss=1.637\n",
      "epoch=385, lrate=0.100, loss=1.636\n",
      "epoch=386, lrate=0.100, loss=1.635\n",
      "epoch=387, lrate=0.100, loss=1.634\n",
      "epoch=388, lrate=0.100, loss=1.633\n",
      "epoch=389, lrate=0.100, loss=1.632\n",
      "epoch=390, lrate=0.100, loss=1.631\n",
      "epoch=391, lrate=0.100, loss=1.630\n",
      "epoch=392, lrate=0.100, loss=1.629\n",
      "epoch=393, lrate=0.100, loss=1.628\n",
      "epoch=394, lrate=0.100, loss=1.627\n",
      "epoch=395, lrate=0.100, loss=1.626\n",
      "epoch=396, lrate=0.100, loss=1.624\n",
      "epoch=397, lrate=0.100, loss=1.623\n",
      "epoch=398, lrate=0.100, loss=1.622\n",
      "epoch=399, lrate=0.100, loss=1.621\n",
      "epoch=400, lrate=0.100, loss=1.620\n",
      "epoch=401, lrate=0.100, loss=1.619\n",
      "epoch=402, lrate=0.100, loss=1.618\n",
      "epoch=403, lrate=0.100, loss=1.617\n",
      "epoch=404, lrate=0.100, loss=1.616\n",
      "epoch=405, lrate=0.100, loss=1.615\n",
      "epoch=406, lrate=0.100, loss=1.614\n",
      "epoch=407, lrate=0.100, loss=1.613\n",
      "epoch=408, lrate=0.100, loss=1.612\n",
      "epoch=409, lrate=0.100, loss=1.611\n",
      "epoch=410, lrate=0.100, loss=1.610\n",
      "epoch=411, lrate=0.100, loss=1.609\n",
      "epoch=412, lrate=0.100, loss=1.608\n",
      "epoch=413, lrate=0.100, loss=1.607\n",
      "epoch=414, lrate=0.100, loss=1.606\n",
      "epoch=415, lrate=0.100, loss=1.605\n",
      "epoch=416, lrate=0.100, loss=1.604\n",
      "epoch=417, lrate=0.100, loss=1.603\n",
      "epoch=418, lrate=0.100, loss=1.602\n",
      "epoch=419, lrate=0.100, loss=1.601\n",
      "epoch=420, lrate=0.100, loss=1.600\n",
      "epoch=421, lrate=0.100, loss=1.599\n",
      "epoch=422, lrate=0.100, loss=1.598\n",
      "epoch=423, lrate=0.100, loss=1.597\n",
      "epoch=424, lrate=0.100, loss=1.596\n",
      "epoch=425, lrate=0.100, loss=1.595\n",
      "epoch=426, lrate=0.100, loss=1.594\n",
      "epoch=427, lrate=0.100, loss=1.593\n",
      "epoch=428, lrate=0.100, loss=1.592\n",
      "epoch=429, lrate=0.100, loss=1.591\n",
      "epoch=430, lrate=0.100, loss=1.590\n",
      "epoch=431, lrate=0.100, loss=1.589\n",
      "epoch=432, lrate=0.100, loss=1.588\n",
      "epoch=433, lrate=0.100, loss=1.587\n",
      "epoch=434, lrate=0.100, loss=1.586\n",
      "epoch=435, lrate=0.100, loss=1.585\n",
      "epoch=436, lrate=0.100, loss=1.584\n",
      "epoch=437, lrate=0.100, loss=1.583\n",
      "epoch=438, lrate=0.100, loss=1.582\n",
      "epoch=439, lrate=0.100, loss=1.581\n",
      "epoch=440, lrate=0.100, loss=1.580\n",
      "epoch=441, lrate=0.100, loss=1.579\n",
      "epoch=442, lrate=0.100, loss=1.579\n",
      "epoch=443, lrate=0.100, loss=1.578\n",
      "epoch=444, lrate=0.100, loss=1.577\n",
      "epoch=445, lrate=0.100, loss=1.576\n",
      "epoch=446, lrate=0.100, loss=1.575\n",
      "epoch=447, lrate=0.100, loss=1.574\n",
      "epoch=448, lrate=0.100, loss=1.573\n",
      "epoch=449, lrate=0.100, loss=1.572\n",
      "epoch=450, lrate=0.100, loss=1.571\n",
      "epoch=451, lrate=0.100, loss=1.570\n",
      "epoch=452, lrate=0.100, loss=1.569\n",
      "epoch=453, lrate=0.100, loss=1.568\n",
      "epoch=454, lrate=0.100, loss=1.567\n",
      "epoch=455, lrate=0.100, loss=1.566\n",
      "epoch=456, lrate=0.100, loss=1.565\n",
      "epoch=457, lrate=0.100, loss=1.564\n",
      "epoch=458, lrate=0.100, loss=1.563\n",
      "epoch=459, lrate=0.100, loss=1.563\n",
      "epoch=460, lrate=0.100, loss=1.562\n",
      "epoch=461, lrate=0.100, loss=1.561\n",
      "epoch=462, lrate=0.100, loss=1.560\n",
      "epoch=463, lrate=0.100, loss=1.559\n",
      "epoch=464, lrate=0.100, loss=1.558\n",
      "epoch=465, lrate=0.100, loss=1.557\n",
      "epoch=466, lrate=0.100, loss=1.556\n",
      "epoch=467, lrate=0.100, loss=1.555\n",
      "epoch=468, lrate=0.100, loss=1.554\n",
      "epoch=469, lrate=0.100, loss=1.553\n",
      "epoch=470, lrate=0.100, loss=1.552\n",
      "epoch=471, lrate=0.100, loss=1.552\n",
      "epoch=472, lrate=0.100, loss=1.551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=473, lrate=0.100, loss=1.550\n",
      "epoch=474, lrate=0.100, loss=1.549\n",
      "epoch=475, lrate=0.100, loss=1.548\n",
      "epoch=476, lrate=0.100, loss=1.547\n",
      "epoch=477, lrate=0.100, loss=1.546\n",
      "epoch=478, lrate=0.100, loss=1.545\n",
      "epoch=479, lrate=0.100, loss=1.544\n",
      "epoch=480, lrate=0.100, loss=1.544\n",
      "epoch=481, lrate=0.100, loss=1.543\n",
      "epoch=482, lrate=0.100, loss=1.542\n",
      "epoch=483, lrate=0.100, loss=1.541\n",
      "epoch=484, lrate=0.100, loss=1.540\n",
      "epoch=485, lrate=0.100, loss=1.539\n",
      "epoch=486, lrate=0.100, loss=1.538\n",
      "epoch=487, lrate=0.100, loss=1.537\n",
      "epoch=488, lrate=0.100, loss=1.536\n",
      "epoch=489, lrate=0.100, loss=1.536\n",
      "epoch=490, lrate=0.100, loss=1.535\n",
      "epoch=491, lrate=0.100, loss=1.534\n",
      "epoch=492, lrate=0.100, loss=1.533\n",
      "epoch=493, lrate=0.100, loss=1.532\n",
      "epoch=494, lrate=0.100, loss=1.531\n",
      "epoch=495, lrate=0.100, loss=1.530\n",
      "epoch=496, lrate=0.100, loss=1.530\n",
      "epoch=497, lrate=0.100, loss=1.529\n",
      "epoch=498, lrate=0.100, loss=1.528\n",
      "epoch=499, lrate=0.100, loss=1.527\n",
      "epoch=500, lrate=0.100, loss=1.526\n",
      "epoch=501, lrate=0.100, loss=1.525\n",
      "epoch=502, lrate=0.100, loss=1.524\n",
      "epoch=503, lrate=0.100, loss=1.524\n",
      "epoch=504, lrate=0.100, loss=1.523\n",
      "epoch=505, lrate=0.100, loss=1.522\n",
      "epoch=506, lrate=0.100, loss=1.521\n",
      "epoch=507, lrate=0.100, loss=1.520\n",
      "epoch=508, lrate=0.100, loss=1.519\n",
      "epoch=509, lrate=0.100, loss=1.518\n",
      "epoch=510, lrate=0.100, loss=1.518\n",
      "epoch=511, lrate=0.100, loss=1.517\n",
      "epoch=512, lrate=0.100, loss=1.516\n",
      "epoch=513, lrate=0.100, loss=1.515\n",
      "epoch=514, lrate=0.100, loss=1.514\n",
      "epoch=515, lrate=0.100, loss=1.513\n",
      "epoch=516, lrate=0.100, loss=1.513\n",
      "epoch=517, lrate=0.100, loss=1.512\n",
      "epoch=518, lrate=0.100, loss=1.511\n",
      "epoch=519, lrate=0.100, loss=1.510\n",
      "epoch=520, lrate=0.100, loss=1.509\n",
      "epoch=521, lrate=0.100, loss=1.508\n",
      "epoch=522, lrate=0.100, loss=1.508\n",
      "epoch=523, lrate=0.100, loss=1.507\n",
      "epoch=524, lrate=0.100, loss=1.506\n",
      "epoch=525, lrate=0.100, loss=1.505\n",
      "epoch=526, lrate=0.100, loss=1.504\n",
      "epoch=527, lrate=0.100, loss=1.504\n",
      "epoch=528, lrate=0.100, loss=1.503\n",
      "epoch=529, lrate=0.100, loss=1.502\n",
      "epoch=530, lrate=0.100, loss=1.501\n",
      "epoch=531, lrate=0.100, loss=1.500\n",
      "epoch=532, lrate=0.100, loss=1.499\n",
      "epoch=533, lrate=0.100, loss=1.499\n",
      "epoch=534, lrate=0.100, loss=1.498\n",
      "epoch=535, lrate=0.100, loss=1.497\n",
      "epoch=536, lrate=0.100, loss=1.496\n",
      "epoch=537, lrate=0.100, loss=1.495\n",
      "epoch=538, lrate=0.100, loss=1.495\n",
      "epoch=539, lrate=0.100, loss=1.494\n",
      "epoch=540, lrate=0.100, loss=1.493\n",
      "epoch=541, lrate=0.100, loss=1.492\n",
      "epoch=542, lrate=0.100, loss=1.491\n",
      "epoch=543, lrate=0.100, loss=1.491\n",
      "epoch=544, lrate=0.100, loss=1.490\n",
      "epoch=545, lrate=0.100, loss=1.489\n",
      "epoch=546, lrate=0.100, loss=1.488\n",
      "epoch=547, lrate=0.100, loss=1.488\n",
      "epoch=548, lrate=0.100, loss=1.487\n",
      "epoch=549, lrate=0.100, loss=1.486\n",
      "epoch=550, lrate=0.100, loss=1.485\n",
      "epoch=551, lrate=0.100, loss=1.484\n",
      "epoch=552, lrate=0.100, loss=1.484\n",
      "epoch=553, lrate=0.100, loss=1.483\n",
      "epoch=554, lrate=0.100, loss=1.482\n",
      "epoch=555, lrate=0.100, loss=1.481\n",
      "epoch=556, lrate=0.100, loss=1.481\n",
      "epoch=557, lrate=0.100, loss=1.480\n",
      "epoch=558, lrate=0.100, loss=1.479\n",
      "epoch=559, lrate=0.100, loss=1.478\n",
      "epoch=560, lrate=0.100, loss=1.477\n",
      "epoch=561, lrate=0.100, loss=1.477\n",
      "epoch=562, lrate=0.100, loss=1.476\n",
      "epoch=563, lrate=0.100, loss=1.475\n",
      "epoch=564, lrate=0.100, loss=1.474\n",
      "epoch=565, lrate=0.100, loss=1.474\n",
      "epoch=566, lrate=0.100, loss=1.473\n",
      "epoch=567, lrate=0.100, loss=1.472\n",
      "epoch=568, lrate=0.100, loss=1.471\n",
      "epoch=569, lrate=0.100, loss=1.471\n",
      "epoch=570, lrate=0.100, loss=1.470\n",
      "epoch=571, lrate=0.100, loss=1.469\n",
      "epoch=572, lrate=0.100, loss=1.468\n",
      "epoch=573, lrate=0.100, loss=1.468\n",
      "epoch=574, lrate=0.100, loss=1.467\n",
      "epoch=575, lrate=0.100, loss=1.466\n",
      "epoch=576, lrate=0.100, loss=1.465\n",
      "epoch=577, lrate=0.100, loss=1.465\n",
      "epoch=578, lrate=0.100, loss=1.464\n",
      "epoch=579, lrate=0.100, loss=1.463\n",
      "epoch=580, lrate=0.100, loss=1.462\n",
      "epoch=581, lrate=0.100, loss=1.462\n",
      "epoch=582, lrate=0.100, loss=1.461\n",
      "epoch=583, lrate=0.100, loss=1.460\n",
      "epoch=584, lrate=0.100, loss=1.459\n",
      "epoch=585, lrate=0.100, loss=1.459\n",
      "epoch=586, lrate=0.100, loss=1.458\n",
      "epoch=587, lrate=0.100, loss=1.457\n",
      "epoch=588, lrate=0.100, loss=1.456\n",
      "epoch=589, lrate=0.100, loss=1.456\n",
      "epoch=590, lrate=0.100, loss=1.455\n",
      "epoch=591, lrate=0.100, loss=1.454\n",
      "epoch=592, lrate=0.100, loss=1.454\n",
      "epoch=593, lrate=0.100, loss=1.453\n",
      "epoch=594, lrate=0.100, loss=1.452\n",
      "epoch=595, lrate=0.100, loss=1.451\n",
      "epoch=596, lrate=0.100, loss=1.451\n",
      "epoch=597, lrate=0.100, loss=1.450\n",
      "epoch=598, lrate=0.100, loss=1.449\n",
      "epoch=599, lrate=0.100, loss=1.449\n",
      "epoch=600, lrate=0.100, loss=1.448\n",
      "epoch=601, lrate=0.100, loss=1.447\n",
      "epoch=602, lrate=0.100, loss=1.446\n",
      "epoch=603, lrate=0.100, loss=1.446\n",
      "epoch=604, lrate=0.100, loss=1.445\n",
      "epoch=605, lrate=0.100, loss=1.444\n",
      "epoch=606, lrate=0.100, loss=1.444\n",
      "epoch=607, lrate=0.100, loss=1.443\n",
      "epoch=608, lrate=0.100, loss=1.442\n",
      "epoch=609, lrate=0.100, loss=1.441\n",
      "epoch=610, lrate=0.100, loss=1.441\n",
      "epoch=611, lrate=0.100, loss=1.440\n",
      "epoch=612, lrate=0.100, loss=1.439\n",
      "epoch=613, lrate=0.100, loss=1.439\n",
      "epoch=614, lrate=0.100, loss=1.438\n",
      "epoch=615, lrate=0.100, loss=1.437\n",
      "epoch=616, lrate=0.100, loss=1.436\n",
      "epoch=617, lrate=0.100, loss=1.436\n",
      "epoch=618, lrate=0.100, loss=1.435\n",
      "epoch=619, lrate=0.100, loss=1.434\n",
      "epoch=620, lrate=0.100, loss=1.434\n",
      "epoch=621, lrate=0.100, loss=1.433\n",
      "epoch=622, lrate=0.100, loss=1.432\n",
      "epoch=623, lrate=0.100, loss=1.432\n",
      "epoch=624, lrate=0.100, loss=1.431\n",
      "epoch=625, lrate=0.100, loss=1.430\n",
      "epoch=626, lrate=0.100, loss=1.430\n",
      "epoch=627, lrate=0.100, loss=1.429\n",
      "epoch=628, lrate=0.100, loss=1.428\n",
      "epoch=629, lrate=0.100, loss=1.428\n",
      "epoch=630, lrate=0.100, loss=1.427\n",
      "epoch=631, lrate=0.100, loss=1.426\n",
      "epoch=632, lrate=0.100, loss=1.425\n",
      "epoch=633, lrate=0.100, loss=1.425\n",
      "epoch=634, lrate=0.100, loss=1.424\n",
      "epoch=635, lrate=0.100, loss=1.423\n",
      "epoch=636, lrate=0.100, loss=1.423\n",
      "epoch=637, lrate=0.100, loss=1.422\n",
      "epoch=638, lrate=0.100, loss=1.421\n",
      "epoch=639, lrate=0.100, loss=1.421\n",
      "epoch=640, lrate=0.100, loss=1.420\n",
      "epoch=641, lrate=0.100, loss=1.419\n",
      "epoch=642, lrate=0.100, loss=1.419\n",
      "epoch=643, lrate=0.100, loss=1.418\n",
      "epoch=644, lrate=0.100, loss=1.417\n",
      "epoch=645, lrate=0.100, loss=1.417\n",
      "epoch=646, lrate=0.100, loss=1.416\n",
      "epoch=647, lrate=0.100, loss=1.415\n",
      "epoch=648, lrate=0.100, loss=1.415\n",
      "epoch=649, lrate=0.100, loss=1.414\n",
      "epoch=650, lrate=0.100, loss=1.413\n",
      "epoch=651, lrate=0.100, loss=1.413\n",
      "epoch=652, lrate=0.100, loss=1.412\n",
      "epoch=653, lrate=0.100, loss=1.411\n",
      "epoch=654, lrate=0.100, loss=1.411\n",
      "epoch=655, lrate=0.100, loss=1.410\n",
      "epoch=656, lrate=0.100, loss=1.410\n",
      "epoch=657, lrate=0.100, loss=1.409\n",
      "epoch=658, lrate=0.100, loss=1.408\n",
      "epoch=659, lrate=0.100, loss=1.408\n",
      "epoch=660, lrate=0.100, loss=1.407\n",
      "epoch=661, lrate=0.100, loss=1.406\n",
      "epoch=662, lrate=0.100, loss=1.406\n",
      "epoch=663, lrate=0.100, loss=1.405\n",
      "epoch=664, lrate=0.100, loss=1.404\n",
      "epoch=665, lrate=0.100, loss=1.404\n",
      "epoch=666, lrate=0.100, loss=1.403\n",
      "epoch=667, lrate=0.100, loss=1.402\n",
      "epoch=668, lrate=0.100, loss=1.402\n",
      "epoch=669, lrate=0.100, loss=1.401\n",
      "epoch=670, lrate=0.100, loss=1.400\n",
      "epoch=671, lrate=0.100, loss=1.400\n",
      "epoch=672, lrate=0.100, loss=1.399\n",
      "epoch=673, lrate=0.100, loss=1.399\n",
      "epoch=674, lrate=0.100, loss=1.398\n",
      "epoch=675, lrate=0.100, loss=1.397\n",
      "epoch=676, lrate=0.100, loss=1.397\n",
      "epoch=677, lrate=0.100, loss=1.396\n",
      "epoch=678, lrate=0.100, loss=1.395\n",
      "epoch=679, lrate=0.100, loss=1.395\n",
      "epoch=680, lrate=0.100, loss=1.394\n",
      "epoch=681, lrate=0.100, loss=1.394\n",
      "epoch=682, lrate=0.100, loss=1.393\n",
      "epoch=683, lrate=0.100, loss=1.392\n",
      "epoch=684, lrate=0.100, loss=1.392\n",
      "epoch=685, lrate=0.100, loss=1.391\n",
      "epoch=686, lrate=0.100, loss=1.390\n",
      "epoch=687, lrate=0.100, loss=1.390\n",
      "epoch=688, lrate=0.100, loss=1.389\n",
      "epoch=689, lrate=0.100, loss=1.389\n",
      "epoch=690, lrate=0.100, loss=1.388\n",
      "epoch=691, lrate=0.100, loss=1.387\n",
      "epoch=692, lrate=0.100, loss=1.387\n",
      "epoch=693, lrate=0.100, loss=1.386\n",
      "epoch=694, lrate=0.100, loss=1.385\n",
      "epoch=695, lrate=0.100, loss=1.385\n",
      "epoch=696, lrate=0.100, loss=1.384\n",
      "epoch=697, lrate=0.100, loss=1.384\n",
      "epoch=698, lrate=0.100, loss=1.383\n",
      "epoch=699, lrate=0.100, loss=1.382\n",
      "epoch=700, lrate=0.100, loss=1.382\n",
      "epoch=701, lrate=0.100, loss=1.381\n",
      "epoch=702, lrate=0.100, loss=1.381\n",
      "epoch=703, lrate=0.100, loss=1.380\n",
      "epoch=704, lrate=0.100, loss=1.379\n",
      "epoch=705, lrate=0.100, loss=1.379\n",
      "epoch=706, lrate=0.100, loss=1.378\n",
      "epoch=707, lrate=0.100, loss=1.378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=708, lrate=0.100, loss=1.377\n",
      "epoch=709, lrate=0.100, loss=1.376\n",
      "epoch=710, lrate=0.100, loss=1.376\n",
      "epoch=711, lrate=0.100, loss=1.375\n",
      "epoch=712, lrate=0.100, loss=1.375\n",
      "epoch=713, lrate=0.100, loss=1.374\n",
      "epoch=714, lrate=0.100, loss=1.373\n",
      "epoch=715, lrate=0.100, loss=1.373\n",
      "epoch=716, lrate=0.100, loss=1.372\n",
      "epoch=717, lrate=0.100, loss=1.372\n",
      "epoch=718, lrate=0.100, loss=1.371\n",
      "epoch=719, lrate=0.100, loss=1.370\n",
      "epoch=720, lrate=0.100, loss=1.370\n",
      "epoch=721, lrate=0.100, loss=1.369\n",
      "epoch=722, lrate=0.100, loss=1.369\n",
      "epoch=723, lrate=0.100, loss=1.368\n",
      "epoch=724, lrate=0.100, loss=1.368\n",
      "epoch=725, lrate=0.100, loss=1.367\n",
      "epoch=726, lrate=0.100, loss=1.366\n",
      "epoch=727, lrate=0.100, loss=1.366\n",
      "epoch=728, lrate=0.100, loss=1.365\n",
      "epoch=729, lrate=0.100, loss=1.365\n",
      "epoch=730, lrate=0.100, loss=1.364\n",
      "epoch=731, lrate=0.100, loss=1.363\n",
      "epoch=732, lrate=0.100, loss=1.363\n",
      "epoch=733, lrate=0.100, loss=1.362\n",
      "epoch=734, lrate=0.100, loss=1.362\n",
      "epoch=735, lrate=0.100, loss=1.361\n",
      "epoch=736, lrate=0.100, loss=1.361\n",
      "epoch=737, lrate=0.100, loss=1.360\n",
      "epoch=738, lrate=0.100, loss=1.359\n",
      "epoch=739, lrate=0.100, loss=1.359\n",
      "epoch=740, lrate=0.100, loss=1.358\n",
      "epoch=741, lrate=0.100, loss=1.358\n",
      "epoch=742, lrate=0.100, loss=1.357\n",
      "epoch=743, lrate=0.100, loss=1.357\n",
      "epoch=744, lrate=0.100, loss=1.356\n",
      "epoch=745, lrate=0.100, loss=1.355\n",
      "epoch=746, lrate=0.100, loss=1.355\n",
      "epoch=747, lrate=0.100, loss=1.354\n",
      "epoch=748, lrate=0.100, loss=1.354\n",
      "epoch=749, lrate=0.100, loss=1.353\n",
      "epoch=750, lrate=0.100, loss=1.353\n",
      "epoch=751, lrate=0.100, loss=1.352\n",
      "epoch=752, lrate=0.100, loss=1.352\n",
      "epoch=753, lrate=0.100, loss=1.351\n",
      "epoch=754, lrate=0.100, loss=1.350\n",
      "epoch=755, lrate=0.100, loss=1.350\n",
      "epoch=756, lrate=0.100, loss=1.349\n",
      "epoch=757, lrate=0.100, loss=1.349\n",
      "epoch=758, lrate=0.100, loss=1.348\n",
      "epoch=759, lrate=0.100, loss=1.348\n",
      "epoch=760, lrate=0.100, loss=1.347\n",
      "epoch=761, lrate=0.100, loss=1.347\n",
      "epoch=762, lrate=0.100, loss=1.346\n",
      "epoch=763, lrate=0.100, loss=1.345\n",
      "epoch=764, lrate=0.100, loss=1.345\n",
      "epoch=765, lrate=0.100, loss=1.344\n",
      "epoch=766, lrate=0.100, loss=1.344\n",
      "epoch=767, lrate=0.100, loss=1.343\n",
      "epoch=768, lrate=0.100, loss=1.343\n",
      "epoch=769, lrate=0.100, loss=1.342\n",
      "epoch=770, lrate=0.100, loss=1.342\n",
      "epoch=771, lrate=0.100, loss=1.341\n",
      "epoch=772, lrate=0.100, loss=1.340\n",
      "epoch=773, lrate=0.100, loss=1.340\n",
      "epoch=774, lrate=0.100, loss=1.339\n",
      "epoch=775, lrate=0.100, loss=1.339\n",
      "epoch=776, lrate=0.100, loss=1.338\n",
      "epoch=777, lrate=0.100, loss=1.338\n",
      "epoch=778, lrate=0.100, loss=1.337\n",
      "epoch=779, lrate=0.100, loss=1.337\n",
      "epoch=780, lrate=0.100, loss=1.336\n",
      "epoch=781, lrate=0.100, loss=1.336\n",
      "epoch=782, lrate=0.100, loss=1.335\n",
      "epoch=783, lrate=0.100, loss=1.335\n",
      "epoch=784, lrate=0.100, loss=1.334\n",
      "epoch=785, lrate=0.100, loss=1.333\n",
      "epoch=786, lrate=0.100, loss=1.333\n",
      "epoch=787, lrate=0.100, loss=1.332\n",
      "epoch=788, lrate=0.100, loss=1.332\n",
      "epoch=789, lrate=0.100, loss=1.331\n",
      "epoch=790, lrate=0.100, loss=1.331\n",
      "epoch=791, lrate=0.100, loss=1.330\n",
      "epoch=792, lrate=0.100, loss=1.330\n",
      "epoch=793, lrate=0.100, loss=1.329\n",
      "epoch=794, lrate=0.100, loss=1.329\n",
      "epoch=795, lrate=0.100, loss=1.328\n",
      "epoch=796, lrate=0.100, loss=1.328\n",
      "epoch=797, lrate=0.100, loss=1.327\n",
      "epoch=798, lrate=0.100, loss=1.327\n",
      "epoch=799, lrate=0.100, loss=1.326\n",
      "epoch=800, lrate=0.100, loss=1.326\n",
      "epoch=801, lrate=0.100, loss=1.325\n",
      "epoch=802, lrate=0.100, loss=1.325\n",
      "epoch=803, lrate=0.100, loss=1.324\n",
      "epoch=804, lrate=0.100, loss=1.324\n",
      "epoch=805, lrate=0.100, loss=1.323\n",
      "epoch=806, lrate=0.100, loss=1.322\n",
      "epoch=807, lrate=0.100, loss=1.322\n",
      "epoch=808, lrate=0.100, loss=1.321\n",
      "epoch=809, lrate=0.100, loss=1.321\n",
      "epoch=810, lrate=0.100, loss=1.320\n",
      "epoch=811, lrate=0.100, loss=1.320\n",
      "epoch=812, lrate=0.100, loss=1.319\n",
      "epoch=813, lrate=0.100, loss=1.319\n",
      "epoch=814, lrate=0.100, loss=1.318\n",
      "epoch=815, lrate=0.100, loss=1.318\n",
      "epoch=816, lrate=0.100, loss=1.317\n",
      "epoch=817, lrate=0.100, loss=1.317\n",
      "epoch=818, lrate=0.100, loss=1.316\n",
      "epoch=819, lrate=0.100, loss=1.316\n",
      "epoch=820, lrate=0.100, loss=1.315\n",
      "epoch=821, lrate=0.100, loss=1.315\n",
      "epoch=822, lrate=0.100, loss=1.314\n",
      "epoch=823, lrate=0.100, loss=1.314\n",
      "epoch=824, lrate=0.100, loss=1.313\n",
      "epoch=825, lrate=0.100, loss=1.313\n",
      "epoch=826, lrate=0.100, loss=1.312\n",
      "epoch=827, lrate=0.100, loss=1.312\n",
      "epoch=828, lrate=0.100, loss=1.311\n",
      "epoch=829, lrate=0.100, loss=1.311\n",
      "epoch=830, lrate=0.100, loss=1.310\n",
      "epoch=831, lrate=0.100, loss=1.310\n",
      "epoch=832, lrate=0.100, loss=1.309\n",
      "epoch=833, lrate=0.100, loss=1.309\n",
      "epoch=834, lrate=0.100, loss=1.308\n",
      "epoch=835, lrate=0.100, loss=1.308\n",
      "epoch=836, lrate=0.100, loss=1.307\n",
      "epoch=837, lrate=0.100, loss=1.307\n",
      "epoch=838, lrate=0.100, loss=1.306\n",
      "epoch=839, lrate=0.100, loss=1.306\n",
      "epoch=840, lrate=0.100, loss=1.305\n",
      "epoch=841, lrate=0.100, loss=1.305\n",
      "epoch=842, lrate=0.100, loss=1.304\n",
      "epoch=843, lrate=0.100, loss=1.304\n",
      "epoch=844, lrate=0.100, loss=1.303\n",
      "epoch=845, lrate=0.100, loss=1.303\n",
      "epoch=846, lrate=0.100, loss=1.302\n",
      "epoch=847, lrate=0.100, loss=1.302\n",
      "epoch=848, lrate=0.100, loss=1.301\n",
      "epoch=849, lrate=0.100, loss=1.301\n",
      "epoch=850, lrate=0.100, loss=1.300\n",
      "epoch=851, lrate=0.100, loss=1.300\n",
      "epoch=852, lrate=0.100, loss=1.299\n",
      "epoch=853, lrate=0.100, loss=1.299\n",
      "epoch=854, lrate=0.100, loss=1.299\n",
      "epoch=855, lrate=0.100, loss=1.298\n",
      "epoch=856, lrate=0.100, loss=1.298\n",
      "epoch=857, lrate=0.100, loss=1.297\n",
      "epoch=858, lrate=0.100, loss=1.297\n",
      "epoch=859, lrate=0.100, loss=1.296\n",
      "epoch=860, lrate=0.100, loss=1.296\n",
      "epoch=861, lrate=0.100, loss=1.295\n",
      "epoch=862, lrate=0.100, loss=1.295\n",
      "epoch=863, lrate=0.100, loss=1.294\n",
      "epoch=864, lrate=0.100, loss=1.294\n",
      "epoch=865, lrate=0.100, loss=1.293\n",
      "epoch=866, lrate=0.100, loss=1.293\n",
      "epoch=867, lrate=0.100, loss=1.292\n",
      "epoch=868, lrate=0.100, loss=1.292\n",
      "epoch=869, lrate=0.100, loss=1.291\n",
      "epoch=870, lrate=0.100, loss=1.291\n",
      "epoch=871, lrate=0.100, loss=1.290\n",
      "epoch=872, lrate=0.100, loss=1.290\n",
      "epoch=873, lrate=0.100, loss=1.289\n",
      "epoch=874, lrate=0.100, loss=1.289\n",
      "epoch=875, lrate=0.100, loss=1.289\n",
      "epoch=876, lrate=0.100, loss=1.288\n",
      "epoch=877, lrate=0.100, loss=1.288\n",
      "epoch=878, lrate=0.100, loss=1.287\n",
      "epoch=879, lrate=0.100, loss=1.287\n",
      "epoch=880, lrate=0.100, loss=1.286\n",
      "epoch=881, lrate=0.100, loss=1.286\n",
      "epoch=882, lrate=0.100, loss=1.285\n",
      "epoch=883, lrate=0.100, loss=1.285\n",
      "epoch=884, lrate=0.100, loss=1.284\n",
      "epoch=885, lrate=0.100, loss=1.284\n",
      "epoch=886, lrate=0.100, loss=1.283\n",
      "epoch=887, lrate=0.100, loss=1.283\n",
      "epoch=888, lrate=0.100, loss=1.283\n",
      "epoch=889, lrate=0.100, loss=1.282\n",
      "epoch=890, lrate=0.100, loss=1.282\n",
      "epoch=891, lrate=0.100, loss=1.281\n",
      "epoch=892, lrate=0.100, loss=1.281\n",
      "epoch=893, lrate=0.100, loss=1.280\n",
      "epoch=894, lrate=0.100, loss=1.280\n",
      "epoch=895, lrate=0.100, loss=1.279\n",
      "epoch=896, lrate=0.100, loss=1.279\n",
      "epoch=897, lrate=0.100, loss=1.278\n",
      "epoch=898, lrate=0.100, loss=1.278\n",
      "epoch=899, lrate=0.100, loss=1.278\n",
      "epoch=900, lrate=0.100, loss=1.277\n",
      "epoch=901, lrate=0.100, loss=1.277\n",
      "epoch=902, lrate=0.100, loss=1.276\n",
      "epoch=903, lrate=0.100, loss=1.276\n",
      "epoch=904, lrate=0.100, loss=1.275\n",
      "epoch=905, lrate=0.100, loss=1.275\n",
      "epoch=906, lrate=0.100, loss=1.274\n",
      "epoch=907, lrate=0.100, loss=1.274\n",
      "epoch=908, lrate=0.100, loss=1.273\n",
      "epoch=909, lrate=0.100, loss=1.273\n",
      "epoch=910, lrate=0.100, loss=1.273\n",
      "epoch=911, lrate=0.100, loss=1.272\n",
      "epoch=912, lrate=0.100, loss=1.272\n",
      "epoch=913, lrate=0.100, loss=1.271\n",
      "epoch=914, lrate=0.100, loss=1.271\n",
      "epoch=915, lrate=0.100, loss=1.270\n",
      "epoch=916, lrate=0.100, loss=1.270\n",
      "epoch=917, lrate=0.100, loss=1.269\n",
      "epoch=918, lrate=0.100, loss=1.269\n",
      "epoch=919, lrate=0.100, loss=1.269\n",
      "epoch=920, lrate=0.100, loss=1.268\n",
      "epoch=921, lrate=0.100, loss=1.268\n",
      "epoch=922, lrate=0.100, loss=1.267\n",
      "epoch=923, lrate=0.100, loss=1.267\n",
      "epoch=924, lrate=0.100, loss=1.266\n",
      "epoch=925, lrate=0.100, loss=1.266\n",
      "epoch=926, lrate=0.100, loss=1.266\n",
      "epoch=927, lrate=0.100, loss=1.265\n",
      "epoch=928, lrate=0.100, loss=1.265\n",
      "epoch=929, lrate=0.100, loss=1.264\n",
      "epoch=930, lrate=0.100, loss=1.264\n",
      "epoch=931, lrate=0.100, loss=1.263\n",
      "epoch=932, lrate=0.100, loss=1.263\n",
      "epoch=933, lrate=0.100, loss=1.262\n",
      "epoch=934, lrate=0.100, loss=1.262\n",
      "epoch=935, lrate=0.100, loss=1.262\n",
      "epoch=936, lrate=0.100, loss=1.261\n",
      "epoch=937, lrate=0.100, loss=1.261\n",
      "epoch=938, lrate=0.100, loss=1.260\n",
      "epoch=939, lrate=0.100, loss=1.260\n",
      "epoch=940, lrate=0.100, loss=1.259\n",
      "epoch=941, lrate=0.100, loss=1.259\n",
      "epoch=942, lrate=0.100, loss=1.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=943, lrate=0.100, loss=1.258\n",
      "epoch=944, lrate=0.100, loss=1.258\n",
      "epoch=945, lrate=0.100, loss=1.257\n",
      "epoch=946, lrate=0.100, loss=1.257\n",
      "epoch=947, lrate=0.100, loss=1.256\n",
      "epoch=948, lrate=0.100, loss=1.256\n",
      "epoch=949, lrate=0.100, loss=1.256\n",
      "epoch=950, lrate=0.100, loss=1.255\n",
      "epoch=951, lrate=0.100, loss=1.255\n",
      "epoch=952, lrate=0.100, loss=1.254\n",
      "epoch=953, lrate=0.100, loss=1.254\n",
      "epoch=954, lrate=0.100, loss=1.254\n",
      "epoch=955, lrate=0.100, loss=1.253\n",
      "epoch=956, lrate=0.100, loss=1.253\n",
      "epoch=957, lrate=0.100, loss=1.252\n",
      "epoch=958, lrate=0.100, loss=1.252\n",
      "epoch=959, lrate=0.100, loss=1.251\n",
      "epoch=960, lrate=0.100, loss=1.251\n",
      "epoch=961, lrate=0.100, loss=1.251\n",
      "epoch=962, lrate=0.100, loss=1.250\n",
      "epoch=963, lrate=0.100, loss=1.250\n",
      "epoch=964, lrate=0.100, loss=1.249\n",
      "epoch=965, lrate=0.100, loss=1.249\n",
      "epoch=966, lrate=0.100, loss=1.249\n",
      "epoch=967, lrate=0.100, loss=1.248\n",
      "epoch=968, lrate=0.100, loss=1.248\n",
      "epoch=969, lrate=0.100, loss=1.247\n",
      "epoch=970, lrate=0.100, loss=1.247\n",
      "epoch=971, lrate=0.100, loss=1.246\n",
      "epoch=972, lrate=0.100, loss=1.246\n",
      "epoch=973, lrate=0.100, loss=1.246\n",
      "epoch=974, lrate=0.100, loss=1.245\n",
      "epoch=975, lrate=0.100, loss=1.245\n",
      "epoch=976, lrate=0.100, loss=1.244\n",
      "epoch=977, lrate=0.100, loss=1.244\n",
      "epoch=978, lrate=0.100, loss=1.244\n",
      "epoch=979, lrate=0.100, loss=1.243\n",
      "epoch=980, lrate=0.100, loss=1.243\n",
      "epoch=981, lrate=0.100, loss=1.242\n",
      "epoch=982, lrate=0.100, loss=1.242\n",
      "epoch=983, lrate=0.100, loss=1.242\n",
      "epoch=984, lrate=0.100, loss=1.241\n",
      "epoch=985, lrate=0.100, loss=1.241\n",
      "epoch=986, lrate=0.100, loss=1.240\n",
      "epoch=987, lrate=0.100, loss=1.240\n",
      "epoch=988, lrate=0.100, loss=1.240\n",
      "epoch=989, lrate=0.100, loss=1.239\n",
      "epoch=990, lrate=0.100, loss=1.239\n",
      "epoch=991, lrate=0.100, loss=1.238\n",
      "epoch=992, lrate=0.100, loss=1.238\n",
      "epoch=993, lrate=0.100, loss=1.238\n",
      "epoch=994, lrate=0.100, loss=1.237\n",
      "epoch=995, lrate=0.100, loss=1.237\n",
      "epoch=996, lrate=0.100, loss=1.236\n",
      "epoch=997, lrate=0.100, loss=1.236\n",
      "epoch=998, lrate=0.100, loss=1.236\n",
      "epoch=999, lrate=0.100, loss=1.235\n"
     ]
    }
   ],
   "source": [
    "batch_size = X_train.shape[0]  #Ideally in powers of 2\n",
    "learning_rate, n_epoch = 0.1, 1000  #Learning rate depends on optimizer\n",
    "# OPTIMIZER CHOICES:\n",
    "# 0: GRADIENT DESCENT(DEFAULT)\n",
    "# 1: GD WITH MOMENTUM\n",
    "# 2: NAG\n",
    "# 3: RMSPROP\n",
    "# 4: ADAGRAD\n",
    "# 5: ADAM\n",
    "optimizer = 0\n",
    "#Train network\n",
    "net.train(X_train, Y_train_enc, learning_rate, n_epoch, batch_size, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b97c47",
   "metadata": {},
   "source": [
    "# Store weights of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "786e042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devansh\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "title = \"[256, 10], ReLu, lr=0.1, n_epoch = 1000\"\n",
    "np.save(title, net.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7d7663",
   "metadata": {},
   "source": [
    "# Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b14d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '[256, 10], tanh, lr=0.0002 batch_size = 64, optimizer = Adam.npy'\n",
    "net.weights = np.load(filename, allow_pickle=True)\n",
    "net.weights = net.weights.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb4952",
   "metadata": {},
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "761d1fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  69.91833333333334\n",
      "Testing accuracy:  70.02000000000001\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = net.predict(X_train)\n",
    "y_pred_test = net.predict(X_test)\n",
    "print(\"Training accuracy: \",accuracy_score(Y_train, y_pred_train)*100)\n",
    "print(\"Testing accuracy: \",accuracy_score(Y_test, y_pred_test)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0275cdbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "0275cdbe",
    "outputId": "f2199b66-1ee8-4185-c5ba-270285cf1fde"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(list(net.loss.keys())).astype(float),np.array(list(net.loss.values())).astype(float))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "P2Q1 - Definitive edition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
